\chapter{Related Work}
\label{chap:relatedwork}

In this chapter we review the literature of research on the topic of software debloating, as well as graph-based representations of the source code commonly used for static analysis, such as call graphs and control flow graphs. 
Lastly, we discuss the literature on symbolic and concolic analysis. 

The idea of software debloating was initially discussed by Zeller et al.~\cite{zeller2002simplifying} as a means to isolate failure-inducing code. 
This idea was later applied to the context of software security to reduce the attack surface of applications and software systems ranging from kernel~\cite{abubakar2021shard}, container environments~\cite{rastogi2017Cimplifier,259711}, binaries~\cite{hasan2022decap, redini2019b, heo2018effective,ghavamnia2020temporal, mishra2020saffire, koo2019configuration, quach2018debloating}, web browsers~\cite{snyder2017most, qian2020slimium}, and web applications~\cite{azad2019less, bulekov2021saphire, mininode, jahanshahi2020you}. 
At a high level, there are three mainstream approaches to debloating: 

\begin{enumerate}
    \item Using static analysis to identify and debloat unreachable code (i.e., dead code)~\cite{redini2019b, snyder2017most, quach2018debloating, mininode, 255308}.
    \item Debloating reachable code which is not exercised under a usage scenario (e.g., automated test cases, runtime dynamic code-coverage traces, and static and dynamic reachability analysis based on web application entry points)~\cite{lessismore, heo2018effective,qian2020slimium, koo2019configuration}.
    \item API specialization, which consists of disabling sensitive APIs or hardening them with respect to the execution context of applications~\cite{mishra2020saffire, saphire, jahanshahi2020you, mishra2021sgxpecial}. 
\end{enumerate}

Orthogonally, we can categorize debloating schemes by their underlying platform. 
We start by reviewing the prior work on debloating for the web including the web servers, web browsers, and web applications. 
Next, we discuss debloating approaches for other platforms, and finally, we go over the literature for module reachability analysis and symbolic execution. 


\section{Debloating for the web}

\paragraph{Debloating web browsers:} Web applications consist of client side and server side modules. 
The client side modules, mainly including HTML, Java-Script and CSS, directly interact with clients' browsers. 
Snyder et al. investigated the idea of API specialization for the JavaScript APIs in web browsers~\cite{snyder2017vibrate}. 
The authors performed a risk analysis for providing access to various browser features and APIs to the JavaScript code.
They evaluated the use of different
JavaScript APIs in the wild and designed a browser extension
to control which APIs any given website would get access to, depending
on that website's level of trust. 
Schwarz et al. similarly utilized a browser
extension to limit the attack surface of Chrome browser and showed that their extension can protect users against micro-architectural and side-channel attacks~\cite{Schwarz2018}. 
Finally, Qian et al. debloated the Chromium browser with respect to the feature usage of specific websites~\cite{qian2020slimium}. 
These studies are orthogonal to our work since
they focus on the client-side aspect of the web, whereas we focus on the server-side web applications. 

\paragraph{Debloating web servers:} Web servers, as the integral part of serving web applications, are prime candidates for debloating. 
Koo et al. explored the debloating of web servers through the analysis of unused features based on a given web server configuration file~\cite{koo2019configuration}. 
Ghavamnia et al. identified that applications such as web servers by design have separate phases (e.g., setup vs serve), and introduced the idea of temporal system call specialization to limit the available APIs based on the current status of the target application. 

\paragraph{Debloating web applications:} For the server-side debloating, Boomsma et al. performed dynamic profiling of a custom web application (a PHP application from an industry partner)~\cite{boomsma2012Dead}. 
The authors measured the time it takes for their dynamic profile system to get
complete coverage and the percentage of files that they could remove. Since they analyzed a proprietary web application, they were not able to report specifics
in terms of the reduction of the programs attack surface, as that relates
to CVEs. 

Koishybayev et al. focused on the bloat from third-party Node.js libraries. 
The authors designed Mininode, which uses static analysis to identify unreachable code in third-party modules and the chain of dependencies in Node.js applications~\cite{mininode}. 
The threat model of Mininode assumes the presence of an arbitrary code execution vulnerability and prevents the attackers from escalating their code execution privileges by accessing sensitive APIs within the main application dependencies. 
Nevertheless, their threat model does not cover the majority of web vulnerabilities including SQLi~\cite{sqlInjection}, XSS~\cite{xss}, CSRF~\cite{csrf}, etc. which by definition are only exploitable from the reachable code. 

\paragraph{API specialization for web applications:} Another group of researchers focused on API specialization to protect web applications against SQL injection attacks~\cite{jahanshahi2020you}.
Orthogonally, Redini et al. proposed BreakApp, which protects Node.js applications by limiting the available APIs for Node.js libraries~\cite{vasilakis2018breakapp} and Jahanshahi et al. designed Saphire, which is their API specialization approach that functions by identifying and limiting the system calls available to each PHP script, thereby limiting the ability of attackers in causing harm~\cite{saphire}. 
Their threat model is similar to Minonode and protects web applications from further exploitation in the presence of a vulnerability that leads to arbitrary code execution. 
API specialization is orthogonal to the debloating schemes discussed in this dissertation and can be applied in parallel to our work to provide further protection for web applications. 


\section{Debloating in other platforms}

The body of research on debloating binaries consists of mechanisms that operate on the source code level, and those that debloat libraries and system calls. 
From the former category, Regehr et al. developed \textit{C-Reduce} which is a tool that performs reduction of C/C++ files by applying very specific program transformation rules~\cite{regehr2012CReduce}.
%Perses
Sun et al. designed a framework called \textit{Perses} that utilizes the grammar of any programming language to guide reduction~\cite{sun2018perses}.
Its advantage is that it does not generate syntactically invalid variants during reduction so that the whole process is made faster.
%Chisel
Heo et al. worked on \textit{Chisel} whose distinguishing feature is that it performs fine-grained debloating by removing code even on the functions that are executed, using reinforcement learning to identify the best reduced program~\cite{heo2018effective}.
%Summary

All three aforementioned approaches are founded on Delta debugging~\cite{zeller2002Delta}.
They reduce the size of an application progressively and verify at each step if the created variant still satisfies the desired properties (e.g., successfully compiles, or passes a set of predefined test cases).

Contrastingly, Sharif et al. proposed \textit{Trimmer}, a system that goes further than simple static analysis~\cite{sharif2018Trimmer}.
It propagates the constants that are defined in program arguments and configuration files so that it can remove code that is not used in that particular execution context.
However, their system is not particularly well suited for web applications where we remove complete features.

Redini et al. utilized abstract interpretation to identify basic blocks within the code that are unused~\cite{redini2019b}. 
They start by building the control flow graph of their target application and removing the disconnected nodes (i.e., basic blocks). 
Mishra et al. explored ways to identify and build profiles of allow-lists for system calls and the parameters passed to them~\cite{mishra2018shredder,mishra2020saffire}. 
Their threat model includes attackers who abuse code execution exploits and limits their ability to abuse the existing code from the applications and disrupts the gadget chains. 

Debloating Kernels, containers and trusted execution environments have also received the attention of researchers~\cite{abubakar2021shard,mishra2021sgxpecial}. 
Rastogi et al. looked at debloating a container by partitioning it into smaller and more secure ones~\cite{rastogi2017Cimplifier}. They perform dynamic analysis on system-call logs to determine which components and executables are used in a container, in order to keep them. 
Ghavamnia et al. looked at this problem from another perspective. 
They proposed Confine, their tool generates system call profiles for containers~\cite{259711}.
Lastly, Abubakar et al. debloated the Linux kernel~\cite{abubakar2021shard} and Mishra et al. proposed an API specialization scheme for Intel SGX APIs~\cite{mishra2021sgxpecial}.

\section{Module reachability analysis}

Control flow graphs (CFG) represent the transitions between the basic blocks of applications, and are commonly used for static analysis. 
Call Graphs (CG) represent the calling relationship between the functions in an application. 
CGs and CFGs can be used for the purpose of debloating by performing a reachability analysis to identify unreachable code as disconnected nodes in the graphs. 
For instance, Redini et al. in their tool Bintrimmer showed that given the CFG of a binary, they can identify disconnected nodes (i.e., dead code) and remove them via debloating~\cite{redini2019b}. 
For web applications specifically, combining CGs with the information about the entry points of applications can be used to identify unreachable, but live code from the exercised entry points. 

The main challenge of call graph generation specially for applications written in dynamic languages such as PHP is the absence of critical information for static analysis that is required to resolve file inclusions and determine the target of dynamic function calls. 
More concretely, PHP allows developers to dynamically decide on which modules to load, and which function to call based on the runtime value of program variables. 

Historically, imperfect call graphs have been used for static analysis in the context of vulnerability discovery. 
Moreover, previous work has addressed the uncertainty of static call graph generation through dynamic analysis. 
For instance, Alhuzaili et al. and Jensen et al. incorporated web crawlers as a pre-analysis step to exercise various parts of the web applications, and as a result they were able to collect dynamic traces of the file inclusions and function calls~\cite{alhuzali2018navex, jensen2012thaps}. 
This way, they can aid their static analysis to build a more accurate call graph. 
Nevertheless, crawlers cannot practically explore all the possible paths within an application. 
Specially for complex applications with multi-stage form submissions, file uploads and form field verifications, web crawlers fall short. 

Orthogonally, Backes et al. proposed the idea of Code Property Graphs (CPGs), which they use to detect PHP vulnerabilities. 
CPGs are generated based on CFGs and CGs. 
In their work, the authors reported that they could only resolve 78.9\% of dynamic function calls. 
While they were able to use the CPGs to report new vulnerabilities, using a call graph with at least 20\% of its edges missing, makes debloating infeasible as it would result in a significant number false positives (i.e., removal of required files and functions). 

An alternative solution devised by researchers to resolve dynamic function calls and file inclusions is to rely on the existing structure of these calls which are commonly made up of multiple string concatenations. 
For instance, Bulekov et al. modeled the file inclusions as regular expressions and mapped them to the underlying file system to identify potential candidate files~\cite{saphire}. 
Similarly, Dahse et al. in their tool RIPS, which incorporates static taint analysis to identify injection style vulnerabilities~\cite{dahse2010rips}, relied on a limited scope variable analysis to resolve the strings used in file inclusions. 
Pixy, one of the first papers to incorporate taint analysis to detect XSS vulnerabilities functions the same way~\cite{jovanovic2006pixy}. 
Overall, we observe that the previous research relies on incomplete call graphs, yet it allows their static analysis tools to identify new vulnerabilities in web applications.

For debloating purposes, an unresolved call site or file inclusion in call graphs leads to error-prone debloating. 
An unresolved call site that can potentially map to any function within the application means that debloating cannot safely reason about and remove any function from the web application. 

\section{Symbolic execution}

The idea of symbolic execution for program testing was originally introduced by King et al.~\cite{king1976symbolic} in 1975. 
The premise of symbolic execution is to explore various execution paths of an application without concretely executing it. 
Symbolic execution can be applied to the binaries (i.e., IR-Less) as well as higher level representations of a program (i.e., IR-Based) such as the LLVM IR~\cite{llvmir}. 
KLEE~\cite{cadar2008klee}, S2E~\cite{chipounov2009selective}, and angr~\cite{cheng2016binary} are IR-based while QSYM~\cite{yun2018qsym}, and SAGE~\cite{godefroid2012sage} are IR-less symbolic analysis engines. 

IR-based engines are easier to implement as they only need to support the fewer IR instructions, and are architecture agnostic. 
Conversely, IR-less implementations are harder to implement and need to support thousands of instructions and are closely tied to the underlying architecture that they support. 
At the same time, IR-less symbolic execution engines provide a better performance~\cite{poeplau2019systematic}.
Symbolic execution engines commonly consist of a symbolic store to include the current variables and their value set and its constraints. 
SMT solvers to evaluate the satisfiability of path conditions~\cite{moura2008z3}. 
State managers to hold the current state of conditions and constraints of the undergoing evaluation along with a set of path constraints. 
By iterating through the instructions of a program, the symbolic execution engine updates the aforementioned structures. 
For instance, if the engine is analyzing a code inside a branch statement, it assumes that the condition of the branch is already satisfied. 

Symbolic execution can be used to automatically generate concrete test cases, as proposed by DART~\cite{sen2009dart, sen2005cute}. 
Alternatively, symbolic execution is proven to be useful for vulnerability analysis (e.g., whether a certain path condition to sensitive program sinks is satisfiable)~\cite{5504701, wang2009intscope, cha2012unleashing, cadar2008klee}. 
Fuzzing tools also benefit from symbolic execution~\cite{godefroid2012sage}.
More interestingly, constraint solvers and variations of symbolic execution has been used for automatic exploit generation (i.e., generating an input that satisfies the path constraints to reach and exploit a known vulnerability)~\cite{alhuzali2018navex, avgerinos2014automatic}.

Concolic execution refers to the interoperation symbolic analysis and concrete execution. 
In this method, programs are executed with concrete values, and parameters from unknown sources (e.g., user input, and the environment) are replaced by symbolic values. 
Concolic execution engines commonly rely on SMT solvers to generate concrete inputs that satisfy the existing path constraints. 
By replacing symbolic values with concrete ones, we reduce the overhead and increase the performance of the analysis. 

In the realm of web applications, Navex used concolic execution as a means for automatic exploit generation~\cite{alhuzali2018navex}. 
One helpful technique used in symbolic web application analysis tools is to isolate the code for their target module before performing the symbolic analysis. 
For instance, Huang et al. built a file upload vulnerability discovery tool named UChecker~\cite{Huang2019}. 
In their tool, they perform an initial phase of static analysis to identify potentially vulnerable sinks. 
Then, through backward slicing, they isolate the code responsible for file upload functionality from user controlled sources to sensitive sinks. 
Similarly, Jensen et al. aid their static analysis by resolving dynamic file inclusions via dynamic analysis by incorporating web crawlers~\cite{jensen2012thaps}. 
They then perform static analysis to identify XSS vulnerabilities and finally validate their findings using symbolic execution. 
In both papers, the authors use symbolic execution to verify the reachability of the vulnerable sinks. 

In a worker closer to ours, Naderi et al. built a PHP emulator and used it in combination with counterfactual execution to uncover obfuscated PHP malware~\cite{naderi2019cubismo,naderi2019malmax}. 
Counterfactual execution is a method of program exploration under which all the loops are unrolled and branches are explored regardless of the satisfiability of their conditions. 
While PHP malware uses specific APIs to remain hidden and perform its malicious actions, complex PHP applications interact with a large number of PHP APIs. 
To be able to perform an end-to-end analysis of these applications, we extended MalMax by adding support for symbolic execution, and implementing the features used in popular PHP applications, which we discuss in more detail in Chapter~\ref{chap:ad}. 

At a high level, symbolic execution challenges can be categorized as follows:
\begin{itemize}
    \item \textbf{State explosion:} Refers to the exponential increase in the number of branches to be explored. This can quickly lead to memory exhaustion, or lack of enough coverage before timeout.
    \item \textbf{Imprecise analysis:} Due to incomplete modeling of the underlying execution environment, the analysis can produce too many false positives or false negatives, or even invalid results. 
    \item \textbf{Constraint solving:} Is a costly task and can lead to timeouts, slowdowns or even unfeasible constraints.
\end{itemize}

To combat the state space explosion problem, researchers have introduced various path prioritization algorithms. 
The simplest form of path exploration is breadth-first-search and depth-first-search, used by DART~\cite{godefroid2005dart}. 
Next to that, other heuristics such as ``reducing the number of redundant path explorations'', ``maximizing code-coverage'', and ``guiding the execution towards security sensitive APIs'' are implemented by researchers in tools such as KLEE~\cite{cadar2008klee}, EXE~\cite{cadar2008exe}, Mayhem~\cite{cha2012unleashing}, S2E~\cite{cha2012unleashing}, and AEG~\cite{avgerinos2014automatic}. 
We incorporate a subset of these mechanisms in our work, which we discuss later in Chapter~\ref{chap:ad}. 
To overcome the second challenge, we designed a feature-complete PHP emulator to closely resemble the outcome of program executions with symbolic variables. 
Finally, we argue that debloating schemes based on concolic execution can benefit from a loose constraint solving without requiring full SMT solvers, we discuss this topic in more detail in Chapter~\ref{chap:ad}. 
