\newpage
\section{Changes in this revision}

This paper was first submitted to the Fall Quarter deadline of USENIX Security. The reviewers decided that we needed to address a number of issues and therefore marked our submission as ``Major Revision.'' In this section, we provide a point-by-point description of the changes that we made according to the reviewers' comments. Where necessary, we also provide clarifications in a rebuttal-like fashion. First, we go over our changes according to the Discussion Summary of our first submission and then describe additional changes, according to the comments of individual reviewers.

\subsection{Discussion Summary}

\noindent $\bullet$ \textit{Discuss limitations of the dynamic analysis tools for application usage profiling (state reachability, input coverage, error code, ...)}
\vspace{0.5ex}

\noindent We have expanded upon the limitations section (Section~\ref{sec:limitations}) to include more details about application-usage profiling. We would like to note that static analysis approaches are not relevant for our work since debloating assumes the presence of code that's reachable but not useful/required. As such, static analysis would greatly overestimate the code that the debloating pipeline needs to maintain. For the same reason, we cannot use test suites because a properly-coded test suite would exercise \emph{all} code of a web application and therefore stop the debloating pipeline from removing any code whatsoever.

\vspace{1ex}

\noindent $\bullet$ \textit{Present deployment scenario and configuration of the web apps (which modules were enabled/disabled?)}
\vspace{0.5ex}

\noindent All web applications were deployed using their default options. We have included a description of this in Section~\ref{sec:cve_reduction}. We further motivate this choice in Point \#4 of the next section of our major-revisions discussion (Section~\ref{sec:additional-comments}).

\vspace{1ex}

\noindent $\bullet$ \textit{Analyze removed code and connect it to the limitations of the tools and/or to the configuration of the web apps under test}
\vspace{0.5ex}

\noindent Given the difficulty of manually analyzing thousands of removed files and somehow communicating the results of this analysis in limited space, we opted for a clustering approach where we cluster removed code based on the file path of each removed file. We describe this clustering and our analysis of the top removed clusters in Section~\ref{subsec:qualitative}.

\vspace{1ex}

\noindent $\bullet$ \textit{Tone down results on effectiveness. Interpret/explain results as the amount of removed code may be overstated.}
\vspace{0.5ex}

\noindent In this submission, we introduced a new web application (WordPress) to our set of evaluated web applications. Evaluating the applicability of debloating to WordPress and obtaining statistics about it took us close to two months (mapping vulnerabilities to source code, coding tutorials as Selenium scripts, and performing the necessary dynamic analysis of the application). Thankfully, we had already started working on WordPress right after our first submission (November 2018) and could therefore obtain these results in time for this submission.

The introduction of WordPress organically toned down our results given that WordPress does not debloat as successfully as the other web applications. This is related to the monolithic nature of WordPress both in terms of files (few source-code files each responsible for a wide range of tasks) as well as functions (functions with a large number of conditional blocks). This new finding further supports our observation that, in terms of debloating, not all applications are ``equal.'' We hope that this observation will inspire different debloating strategies that lend themselves better to monolithic web applications, as well as systems that can predict the ``debloatability'' of a web application, without having to go through the process of mapping vulnerabilities and obtaining usage traces. Finally, we also toned down the effectiveness results for the remaining web applications, always pointing the reader to the limitations of dynamic analysis (Section~\ref{sec:limitations}).

\vspace{1ex}

\noindent $\bullet$ \textit{Test exploitability of vulnerabilities before debloating.}
\vspace{0.5ex}

\noindent We added additional exploits for the WordPress web application (the new web application that we evaluated in this version of the paper) and confirmed that we could successfully launch them before debloating each web application. This change is reflected in the text of Section~\ref{section:metasploit}.

\vspace{1ex}

\noindent $\bullet$ \textit{Rerun monkey tester post-debloating to identify whether any scenarios are broken.}
\vspace{0.5ex}

\noindent We chose a constant seed for our monkey-testing tools, allowing us to conduct monkey testing \textit{before} and \textit{after} the debloating stage. This change is reflected in the text of Section~\ref{sec:monkey}.

\vspace{1ex}

\noindent $\bullet$ \textit{Avoid using LOCs as the metric, this is potentially misleading.}
\vspace{0.5ex}

\noindent We would like to point out that in our paper, we are using LLOC (Logical Lines of Code) as opposed to LOC (Lines of Code) which, as described in Section~\ref{subsubsec:lloc}, does not count comments and newlines and is not affected by syntactic structures of the language. We were always using LLOC exactly because LOC can be misleading and can exaggerate the performance of debloating. Given prior work on the positive correlations of LLOC and vulnerabilities (according to McConnel~\cite{mcconnell2004code}, the industry average, at least in 2004, was to have between 1 and 25 bugs for every one thousands lines of code), we argue that showing the reduction in LLOC is a statistic that allows cross-application comparison and therefore is meaningful. At the same time, in this new version of the paper, we are analyzing the removed code (Section~\ref{subsec:qualitative}) to allow readers to better understand exactly what was removed, without having to manually look at thousands of source-code files.


\subsection{Additional Comments}
\label{sec:additional-comments}

\noindent \textbf{1. Reviewer B:} \textit{Is it practical to deploy this sort of coverage measurement on live traffic? Whats the overhead?}
\vspace{0.5ex}

\noindent Even though the performance overhead of running code-coverage tools on the server was not mentioned in the ``Discussion Summary'' section, Reviewer B had inquired about it and we had promised to conduct these experiments for our camera ready. Section~\ref{subsection:performance} presents the server-side overhead of the current coverage-measuring techniques.

\vspace{1ex}


\noindent \textbf{2. Reviewer C:} \textit{The authors include a list of URLs to the tutorials, which is good. However, I think this is not sufficient, and it would have been more informative an analysis of the workflows covered by these tutorials highlighting the exercised functionalities.}
\vspace{0.5ex}

\noindent To help readers understand what actions the tutorials covered, we added a high-level description of these actions in Section~\ref{sec:tutorials}. Unfortunately, we cannot go into any more detail in the paper, without turning that section into a long list of bullet points. We hope that our high-level discussion and the links to exact URLs that were followed (in addition to all the artifacts that we will release together with this paper) will allow readers to gauge the coverage of the tutorials.

At the same time, we cannot guarantee that all features that all readers deem important will be present in these tutorials. From the beginning of this study, we decided against making these value-judgments ourselves since we could bias the experiments. Our reliance on tutorials, monkey-testing, crawling, and (added in this submission) vulnerability scanning, while imperfect, allows us to objectively measure the coverage and the ``debloatability'' of the evaluated web applications. Further research using real data from web application deployments is required to more accurately measure the needs of individual deployments and how the coverage obtained from these deployments compares to the coverage presented in this paper.

\vspace{1ex}

\noindent \textbf{3. Reviewer C:} \textit{Third, I could not see a mention of the code handling exceptional behaviors and errors. I think these aspects are important too and should be included in the deployed code. How did the authors trigger these behaviors?}
\vspace{0.5ex}

\noindent In our previous submission we indeed relied on monkey-testing to increase the coverage of error-handling code and maintain that code during the process of debloating. To increase coverage, in this submission, we included a vulnerability scanner as a fourth application-stimulation method. Unlike monkey-testing tools, vulnerability scanners attempt to construct a wide range of malicious inputs in order to find vulnerabilities such as SQL injections and Cross-Site Scripting attacks. These inputs allow us to trigger more error-handling, server-side code compared to previous approaches.

\vspace{1ex}

\noindent \textbf{4. Reviewer C:} \textit{My concern here is that the authors used the entire source code as ground truth and neglected the role played by the hundreds of configuration parameters, options, and modules that could change the amount of code exposed and executed by users.}
\vspace{0.5ex}

\noindent Some of the evaluated web applications are highly configurable and as Reviewer C mentioned, could very well have hundreds of different configuration parameters. As with the discussion about code coverage, it is impossible for us to choose these parameters in an objective fashion. As such, we decided not to choose at all and relied on defaults for all deployed web applications. We would like to note that even if a module is ``disabled'', the code still resides on the server and could be abused by specific types of attacks. For example, in a recent attack against a WordPress plugin, the vulnerability could be exploited even if the vulnerable plugin was disabled~\cite{wordpressPlugin}.

\vspace{1ex}

\noindent \textbf{5. Reviewer C:} \textit{The three web applications are relevant ones. However, I could not understand why the authors focused on different versions of the same web applications and the added value of such a decision. Three web applications is well below the current best-practices in web application security and I feel the paper would have been much stronger if the authors had considered to test more web applications.}
\vspace{0.5ex}

\noindent Given that not all versions of any given web application are vulnerable to a given CVE, we needed to use multiple versions of a web application to ensure that we could map each application's CVEs, as described in Section~\ref{sec:vulntosourcemapping} and listed in Table~\ref{table:listofallcves}. For this new submission, we have included WordPress and an additional 20 CVEs. As we mentioned earlier, adding support for more web applications is not trivial and takes anywhere between one-to-two months to map CVEs to source code, encode tutorials as Selenium scripts, and run the applications through our debloating pipeline. We could only add WordPress because we had started working on it in November 2018, right after our initial submission. We hope that once this work is published, it will motivate future research that can reduce the amount of time necessary for the aforementioned, labor-intensive tasks.

\vspace{1ex}

\noindent \textbf{6. Reviewer D:} \textit{It's not clear if you are counting test files. Many web apps (and packages) just include a test directory; these files will be trivially removed by your tool. Did you remove them ahead of time or is your counting a bit off?}
\vspace{0.5ex}

\noindent We have quantified the effect of test files in Section~\ref{subsec:qualitative}. Test files are either absent or their footprint is minor (compared to the rest of the code) for most applications in our testbed (with the exception of Magento 2.0.5). At the same time, test files may contain vulnerabilities of their own and are, in our subjective experience setting up and administering web applications, not deleted by administrators (out of fear of breaking the web application). As such, we consider it appropriate to keep them in the original applications and allow the debloating pipeline to automatically remove them.

\vspace{1ex}

\noindent  \textbf{7. Reviewer D:} \textit{Using the selenium-mapped tutorials to test whether or not you broke functionality is not sufficient. Or at the very least it is biased and probably not exhaustive.  I recognize that this is a hard problem (and indeed a limitation you bring up), but why not throw the crawler and/or monkey testing to introduce some randomness?}
\vspace{0.5ex}

\noindent Selenium-mapped tutorials are not meant to be exhaustive. The entire premise of debloating (or attack-surface reduction) is that not all users use applications in the same way and therefore, in any given application deployment, there exist unused features that are contributing to the overall vulnerability of an application, without providing any utility to users. The Selenium tutorials that we encoded allows us to compute an \textit{objective} approximation of the features that real users would rely on. Crawling, monkey testing, and (in this submission) vulnerability scanning, are meant to increase the code that is being used and enable us to be more conservative when removing server-side code during the debloating stage.

\vspace{1ex}

\noindent  \textbf{8. Reviewer D:} \textit{A number of times you claim that debloating reduces the attack surface or that using packages comes at the cost of attack surface. I believe that debloating reduces the number of CVEs and POIs, but are all the CVEs and POIs actually on the surface?
You can't say things like ``every new feature further expands a program's attack surface''; this can't possibly be true.}
\vspace{0.5ex}

\noindent There seems to be a misunderstanding about the definition of the ``attack surface'' term. We use the definition by Pfleeger et al.~\cite{pfleeger2015security} where a system's attack surface is ``the system's full set of vulnerabilities --- actual and potential ones.'' OWASP and the SANS institute offer similar, albeit more verbose, definitions~\cite{owasp-attacksurface,sans-attacksurface}. This means that, other things being equal, larger applications have a larger attack surface than smaller ones. This is not a controversial statement and has been measured extensively in the software industry~\cite{mcconnell2004code}. To add new features to a program one needs to add more code, which therefore means that every new feature indeed expands a program's attack surface. We believe that Reviewer D thought that we are making claims about whether bugs are ``shallow'' (i.e., on the surface) vs. ``deep'' but that is not what we are discussing or evaluating.

\vspace{1ex}

\noindent  \textbf{9. Reviewer D:} \textit{On pg 3 you suggest debloating as a way to remove vulnerabilities. This seems a bit silly. First, your tool clearly misses some vulnerabilities. Second, this seems way more complicated than a CVE scanner that tells the developer to update a dependency.}
\vspace{0.5ex}

\noindent Unfortunately there again appears to be a misunderstanding about what exactly we are doing in this paper. The process of debloating will remove code that users do not require. Statistically, this code will include vulnerabilities that are, at the time of debloating, unknown. As such, when one or more vulnerabilities are later discovered, the application will not be vulnerable to them because the code that includes the vulnerabilities is altogether absent from the application.

All of the above is straightforward and is essentially the theory behind all attack-surface reduction/debloating approaches. The novelty of our work relies on evaluating how well debloating performs for web applications (which had not been previously evaluated). Given that we cannot predict the future, we chose historical CVEs (in addition to LLOC, gadgets, and other code-quality metrics) to measure how well popular web applications debloat and how many vulnerabilities users would be safe from, had they debloated their applications. In other words, one would use debloating to protect themselves against future unknown bugs, not current ones for which patches exist.
